# QuerySage ðŸ¤–ðŸ“š  
An AI-powered chatbot built with **LLaMA 3 (local inference)** + **FAISS vector database** + **Flask backend** + **Streamlit frontend**.  

QuerySage is designed to work as your personal AI assistant with **document understanding** â€“ you can upload PDFs, TXT, or images (OCR-ready) and get instant answers based on the content.  

---

## âœ¨ Features
- ðŸ”— **RAG (Retrieval Augmented Generation)** â€“ query over your own PDFs, TXT, or files.
- âš¡ **Local Inference** â€“ runs on **LLaMA 3** locally (no API costs, no data leakage).
- ðŸ“Š **FAISS Vector Store** â€“ fast and efficient similarity search.
- ðŸ–¥ **Streamlit Frontend** â€“ clean, ChatGPT-like UI branded as *QuerySage*.
- ðŸ§© **Flask Backend** â€“ lightweight REST API serving model and document ingestion.
- ðŸ“‚ **Dynamic File Uploads** â€“ instantly ingest new files without rebuilding the vector DB.
- ðŸ›¡ **Customizable** â€“ swap models, change chunk size, or embeddings.

---

## ðŸ“‚ Project Structure
QuerySage/
â”‚
â”œâ”€â”€ backend/
â”‚ â”œâ”€â”€ app/
â”‚ â”‚ â”œâ”€â”€ main.py # Flask API
â”‚ â”‚ â”œâ”€â”€ ingestion.py # Vector store creation
â”‚ â”‚ â”œâ”€â”€ embeddings.py # SentenceTransformers embeddings
â”‚ â”‚ â”œâ”€â”€ config.py # Config/env setup
â”‚ â””â”€â”€ models/llama-3/ # Local GGUF LLaMA model
â”‚
â”œâ”€â”€ frontend/
â”‚ â”œâ”€â”€ app.py # Streamlit app (ChatGPT-style UI)
â”‚
â”œâ”€â”€ data/
â”‚ â””â”€â”€ vectordb/ # FAISS index + stored docs
â”‚
â”œâ”€â”€ .env # Model paths, configs
â””â”€â”€ requirements.txt

yaml
Copy code

---

## ðŸš€ Getting Started

### 1. Clone Repo
```bash
git clone https://github.com/your-username/QuerySage.git
cd QuerySage
2. Setup Environment
bash
Copy code
conda create -n querysage-env python=3.10 -y
conda activate querysage-env
pip install -r requirements.txt
3. Configure Environment
Create a .env file:

ini
Copy code
MODEL_PATH=./models/llama-3/Llama-3.1-8B-Mistral-7B-v0.3-mix.IQ3_M.gguf
VECTOR_DB_PATH=./data/vectordb
CHUNK_SIZE=500
4. Run Backend
bash
Copy code
cd backend
python app/main.py
Server runs at â†’ http://127.0.0.1:8000

5. Run Frontend
bash
Copy code
cd frontend
streamlit run app.py
Frontend runs at â†’ http://localhost:8501

ðŸ“– Usage
Ask questions directly in chat (general knowledge via LLaMA).

Upload PDF/TXT â†’ instantly ingested into FAISS â†’ chatbot answers from the file.

Soon: image-to-text (OCR) ingestion ðŸš€.

ðŸ›  Tech Stack
Model: LLaMA 3 (GGUF with llama.cpp)

Vector DB: FAISS

Embeddings: SentenceTransformers (all-MiniLM-L6-v2)

Backend: Flask

Frontend: Streamlit

Language: Python 3.10

ðŸ“¸ Demo

ðŸŒŸ Future Enhancements
Add OCR for scanned PDFs & images.

Deploy with Docker.

Add multi-user chat sessions.

Integrate advanced LLaMA quantization for faster inference.

ðŸ™Œ Acknowledgments
Meta AI â€“ LLaMA 3

llama.cpp

SentenceTransformers

FAISS
